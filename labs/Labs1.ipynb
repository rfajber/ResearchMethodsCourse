{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the packages that I used making when I solved these problems\n",
    "# you might not have to use all of them \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as la\n",
    "import scipy.stats as st\n",
    "import xarray as xr\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.1\n",
    "\n",
    "## The Central Limit Theorem: \n",
    "1. generate M variables that are the sum of N randomly drawn uniform variables and plot their distribution. Choose N and M each to be [10,100,1000]. \n",
    "2. Compare the distributions that you are computing to what the theoretical distribution should be.\n",
    "\n",
    "Hint: For a uniform distribution between $(x_0, x_1)$ the mean is $0.5(x_0+x_1)$ and the variance is $(x_0,x_1)^2/12$. What will the mean and variance be of the sum of independent uniform variables?\n",
    "\n",
    "Hint: you should think about what the normalization here is. For the theoretical distribution you are plotting you will have $\\int P(x) dx = 1$. Is that true for the distribution that you are sampling from the data?\n",
    "\n",
    "Note: efficient algorithms for sampling uniform variables were invented before algorithms for sampling Gaussian variables, so this algorithm with m=12 used to be one of the main choices for sampling Gaussian variables.\n",
    "\n",
    "## Calculating statistics from data \n",
    "   1. open the 2m temperature data in the netcdf files in data/cities. For each city calculate the monthly mean and standard deviation of the 2m temperature. \n",
    "\n",
    "Hint: theres a few ways to do this: \n",
    "- simple: write a loop over the data arrays and select each month\n",
    "- intermediate: load the data into a numpy array and use np.reshape to turn an array from [time] to [mon,year] and then use array operations.\n",
    "- advanced: add a new monthly index and use the xarray group-by combine feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2 \n",
    "\n",
    "## Correlation and Independence \n",
    "1. Load the data in the aptly named FakeData.nc \n",
    "2. Compute the covariance and correlation coeffecient between y1,y2,y3 and x. \n",
    "3. For each y decide if the data is uncorrelated and/or independent of the variable x. Why or why not?\n",
    "   \n",
    "Hint: you might want to look at plots of the variables.\n",
    "   \n",
    "## Calculating second order statistics\n",
    "1. Open the 2m temperature data in the netcdf files in data/cities. \n",
    "2. For each pair of cities calculate the annual mean covariance and correlation and display it\n",
    "3. For each pair of cities calculate the monthly mean covariance and correlation and display it \n",
    "4. What is the difference between the covariance and correlation?\n",
    "   \n",
    "Hint: you should reuse your code from the previous lab!\n",
    "   \n",
    "Hint: there are many different ways to display this, one way is to plot the correlation matrix using plt.pcolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.3 \n",
    "\n",
    "## Confidence Intervals \n",
    "1. Once again, load the temperature data in data/cities\n",
    "2. calculate the p=0.1 confidence intervals and plot these around the seasonal means \n",
    "3. how do these plots relate to a hypothesis test that the seasonal means are different?\n",
    "\n",
    "Hint: plt.fill_between is a useful function for plotting filled regions\n",
    "\n",
    "\n",
    "## Some basic hypothesis testing \n",
    "1. calculate the pairwise correlation between the annual mean temperature of all the cities and plot it.\n",
    "2. Add hatching to the plot to show the correlation values that are statistically significantly different from 0 at the p=0.05 level.  \n",
    "\n",
    "Hint: plt.gca().add_patch(Rectangle((x, y), 1, 1, fill=False, hatch='l')) will plot a hatched rectangle of unit width with lower left corner at (x,y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.4\n",
    "\n",
    "A very simple model for globally averaged temperature change is \n",
    "\n",
    "$$ C \\frac{dT}{dt} = \\lambda {T} + \\mathrm{ERF}  $$\n",
    "\n",
    "the left hand side is the rate of change of energy storage in the climate system, the first term on the right hand side is the feedback term, and the second term on the right hand side is the \"effective radiative forcing\", which is the rate of energy storage caused by either greenhouse gases or volcanic emissions changing the radiative balance at the surface. You don't really have to know this model in any detail, but its useful to understand what the ERF is. \n",
    "\n",
    "The file AR6_ERF_1750-2019.csv contains the ERF time series for different forcing agents from the IPCC AR6 report estimates. These are our best understanding of the radiative impacts of different gases and aerosols.\n",
    "\n",
    "The file 1850-2022.csv contains a global surface temperature anomalies relative to a 1971â€“2000 base period from the NOAAGlobalTemp dataset.\n",
    "\n",
    "1. load the  file data/global/AR6_ERF_1850-2022.csv and plot the global temperature anomalies\n",
    "2. perform a linear regression for the global temperature against time. \n",
    "   - Calculate the confidence intervals for your regression coefficients. Is the trend statistically significant at the p=0.05 level?\n",
    "   - Is there anything you could do to improve the statistical significance of the trend? Should you do it if there is?  \n",
    "3. load the file data/global/AR6_ERF_1750-2019.csv and plot the total_natural and total_anthropogenic ERF time series from 1850-2019.\n",
    "4. Use the total_natural and total_anthropogenic ERF time series as predictors and redo the regression. Plot both the regression estimates and actual data as functions of time. What do you notice about this solution compared to the previous one?\n",
    "   - calculate the confidence intervals for the regression parameters. Are they significant?\n",
    "5. Redo the previous calculation but using either the total_natural xor the total_anthropogenic ERF time series. \n",
    "   - calculate the confidence intervals for the regression parameters. Are they significant? What can you conclude about global warming?\n",
    "\n",
    "Hint: Remember to append a row of ones to your matrix in order to ensure that you are including a constant term in your regression.\n",
    "\n",
    "Hint: you can reduce the amount of code you need to write with a function that takes in the predictors and then calculates all the outputs you need in order to completely answer each question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
