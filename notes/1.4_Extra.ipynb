{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good is the model?\n",
    "- We can start by manipulating the total variance of y is: $$ \\begin{align*} s_y^2 &=& \\overline{\\left(y-\\overline{y}\\right)^2} \\\\ &=& \\overline{\\left[\\left(y-\\hat{y}\\right)-\\left(\\hat{y}-\\overline{y}\\right)\\right]^2}\\\\ \\end{align*} $$\n",
    "- $\\overline{\\hat{y}}=\\overline{y}\\implies \\hat{y}-\\overline{y} = \\hat{y}' $\n",
    "- $y-\\overline{\\hat{y}}=\\epsilon, \\overline{\\hat{y}}=\\overline{y}\\implies y-\\overline{\\hat{y}}=\\epsilon' $\n",
    "\n",
    "- $$s_y^2 = \\overline{\\left(\\epsilon'-\\hat{y}'\\right)^2}=\\overline{\\epsilon'^2}+2\\overline{\\epsilon'\\hat{y}'}+\\overline{\\hat{y}'^2}$$\n",
    "\n",
    "- for middle term, $\\overline{\\epsilon'\\hat{y}'}=0$ (see derivation of $m$)\n",
    "\n",
    "- for last term $\\overline{\\hat{y}_i'^2}=m^2\\overline{x_i'^2}=\\left(\\rho s_y s_x^{-1}\\right)^2s_x^2=\\rho^2s_y^2$\n",
    "\n",
    "- And so finally get $$ 1 = \\frac{\\overline{\\epsilon'^2}}{s_y^2}+\\rho^2 $$\n",
    "\n",
    "- Could also rewrite: $\\overline{\\epsilon'^2} = \\left(1-\\rho^2\\right)s_y^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\chi^2$ Tests \n",
    "\n",
    "- Here we minimized an objective function $$J=\\sum_i \\epsilon_i^2$$\n",
    "- What happens if each observation has different errors? Then we could minimize an objective function $$\\chi^2=\\sum_i \\frac{\\epsilon_i^2}{r_i^2}$$ where $r_i$ is an estimate of the error that can vary between measurements \n",
    "- In matrix form, $$\\chi^2 = \\left(\\vec{y}-\\hat{\\vec{y}}\\right)^T R \\left(\\vec{y}-\\hat{\\vec{y}}\\right)$$ Where $R$ is a error covariance matrix. \n",
    "- We will come back to this in section 5, since it plays an important role in data assimilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Regressors\n",
    "\n",
    "- The name linear regression is sort of a lie, what we really mean is that the model is linear between the estimate of the model, $\\hat{y}$, and the input, x. \n",
    "\n",
    "- one way is to think about this in time. Imagine that we have $y(t)$ and $x(t)$. $x(t)$ and $y(t)$ might both be non-linear, but if $y=mx$, then we could still use linear regression. \n",
    "\n",
    "- when we have multiple inputs that vary with time, one trick is to insert 1 input variable that does not vary with time in order to esimate a constant. \n",
    "\n",
    "\n",
    "$$ \\vec{\\hat{y}} = \\begin{pmatrix} t_1 & 1 \\\\ t_2 & 1 \\\\ ... & ... \\\\ t_n & 1  \\end{pmatrix} \\begin{pmatrix} m \\\\ b \\end{pmatrix}  = \\begin{pmatrix} mt_1 +b \\\\ m t_2 +b \\\\ ...  \\\\ m t_n + b  \\end{pmatrix} $$\n",
    "\n",
    "- If we thought that $y=m e^t + b $ and wanted to find $m$ and $b$ we could have instead done \n",
    "\n",
    "$$ \\vec{\\hat{y}} = \\begin{pmatrix} e^{t_1} & 1 \\\\ e^{t_2} & 1 \\\\ ... & ... \\\\ e^{t_n} & 1  \\end{pmatrix} \\begin{pmatrix} m \\\\ b \\end{pmatrix}  = \\begin{pmatrix} me^{t_1} +b \\\\ m e^{t_2} +b \\\\ ...  \\\\ m e^{t_n} + b  \\end{pmatrix} $$\n",
    "\n",
    "### Note - this is an error in the notes (but not in the code)\n",
    "\n",
    "- To include a constant term in a multivariate regression problem you simply append an extra constant predictor, making X and Nx(M+1) matrix, e.g. \n",
    "$$ X = \\left(\\begin{array}{ccccc}\n",
    "x_{11} & x_{12} & ... &  & x_{1M} & 1\\\\\n",
    "\\\\\n",
    "\\vdots &  & x_{ij} &  & \\vdots & 1\\\\\n",
    "\\\\\n",
    "x_{N1} & ... &  &  & x_{NM} &1  \\\\\n",
    "\\end{array}\\right) $$ \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
